function [w_o_stage1_trained, E_final, current_mse] = trainOutputLayer_GD_Autograd(X, T, w_initial, ...
                                                      max_epochs, eta_output, batch_size)
% trainOutputLayer_Adam: Çıktı katmanını 'Mini-Batch' ve 'ADAM' optimizatörü ile eğitir.
% 'Adam', (Quickprop gibi) 'momentum' ve 'uyarlanabilir öğrenme oranını' birleştirir.
N = size(X, 1);
if N == 0
    error('Giriş matrisi (X) boş olamaz.');
end
% --- Başlangıç Değerlerini Ayarla ---
eta = eta_output; % Adam için temel öğrenme oranı
% <<< DEĞİŞİKLİK 1: Ağırlıkları 'dlarray' yap >>>
w_o_stage1_trained = dlarray(w_initial); 
% <<< DEĞİŞİKLİK 2: Adam optimizatörünün 'durum' (state) değişkenlerini başlat >>>
% Bunlar, Quickprop'taki 'prev_dw_o' ve 'prev_grad_o'nun modern karşılığıdır.
avg_grad = []; 
avg_sq_grad = []; 

% <<< YENİ KOD: Toplam iterasyon sayacını BURADA başlatın >>>
iteration_counter = 0; 

fprintf('Çıktı katmanı eğitimi (Mini-Batch Adam Autograd) başlıyor...\n');
for epoch = 1:max_epochs
    
    % ... (Veri karıştırma kısmı aynı) ...
    indices = randperm(N);
    X_shuffled = X(indices, :);
    T_shuffled = T(indices, :);
    
    X_dl = dlarray(X_shuffled);
    T_dl = dlarray(T_shuffled);
    
    for i = 1:batch_size:N
        end_idx = min(i + batch_size - 1, N);
        batch_indices = i:end_idx;
        
        X_batch_dl = X_dl(batch_indices, :);
        T_batch_dl = T_dl(batch_indices, :);
        
        % 1. Gradyanı 'dlfeval' ile OTOMATİK hesapla
        grad_o_batch = dlfeval(@modelGradient, w_o_stage1_trained, X_batch_dl, T_batch_dl);
        
        % <<< YENİ KOD: Her batch işlendiğinde sayacı artır >>>
        iteration_counter = iteration_counter + 1;
        
        % <<< DEĞİŞİKLİK: 'epoch' yerine 'iteration_counter' kullanın >>>
        % 2. Ağırlık Güncelleme (Adam Optimizatörü)
        [w_o_stage1_trained, avg_grad, avg_sq_grad] = adamupdate(w_o_stage1_trained, grad_o_batch, ...
                                    avg_grad, avg_sq_grad, iteration_counter, eta);
    end
end
% --- FİNAL ÇIKTILARINI AYARLA ---
w_final_double = extractdata(w_o_stage1_trained);
Y_pred_final = X * w_final_double; 

% DÜZELTME (Değişken adları imza ile eşleşmeli):
E_final = T - Y_pred_final;
final_mse = 0.5 * mean(E_final(:).^2);

fprintf('Çıktı katmanı (Mini-Batch Autograd) tamamlandı. Son MSE: %f\n', final_mse);
end % Fonksiyonun sonu
% --- YARDIMCI FONKSİYON (Değişmedi) ---
function [gradients] = modelGradient(w, X_batch, T_batch)
    Y_pred_batch = X_batch * w;
    E_batch = T_batch - Y_pred_batch;
    loss = 0.5 * mean(E_batch(:).^2); 
    gradients = dlgradient(loss, w);
end