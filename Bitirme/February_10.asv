%% CCNN - RECURSIVE N-STEP (Trajectory Loss)
% MODEL: N-step
% CANDIDATE: N-step
% (Fully consistent CCNN for system identification)

clear; clc; close all; rng(0);

%% =========================
%% 0) CONFIG
%% =========================
config = struct();

config.data.source = 'twotankdata';
config.data.twotank.filter_cutoff = 0.066902;
config.data.twotank.warmup_samples = 20;
config.data.twotank.sampling_time = 0.2;

config.data.train_ratio = 0.5;
config.data.val_ratio   = 0.5;

config.norm_method = 'ZScore';

config.prediction.n_steps = 480;
config.validation.n_steps = 500;




config.regressors.include_bias = false;

config.model.max_hidden_units   = 10;
config.model.target_mse         = 5e-5;

config.model.max_epochs_output    = 100;
config.model.eta_output           = 0.02;

config.model.max_epochs_candidate = 100;
config.model.eta_candidate        = 0.02;

config.model.activation = 'tanh';

%% =========================
%% 1) LOAD DATA
%% =========================
[Utr_raw, Ytr_raw, Uva_raw, Yva_raw] = loadDataByConfig_min(config);

%% =========================
%% 2) NORMALIZE
%% =========================
[Utr, Ytr, Uva, Yva, norm_stats] = normalizeData_min( ...
    config.norm_method, Utr_raw, Ytr_raw, Uva_raw, Yva_raw);

%% =========================
%% 3) TRAJECTORY DATASET
%% =========================
Npred = config.prediction.n_steps;

[X0_tr, Utr_seq, Ttr_seq] = createTrajectoryDataset(Utr, Ytr, config, Npred);
[X0_va, Uva_seq, Tva_seq] = createTrajectoryDataset(Uva, Yva, config, Npred);

%% =========================
%% 4) ACTIVATION
%% =========================
g = @(x) tanh(x);

%% =========================
%% 5) TRAIN CCNN (N-STEP MODEL + N-STEP CANDIDATE)
%% =========================
fprintf('\n=== CCNN TRAINING (N-STEP MODEL + N-STEP CANDIDATE) ===\n');

W_hidden = {};
d0 = size(X0_tr,2);
w_o = randn(d0,1)*0.01;

% ---- Stage 1: output only ----
[w_o, current_mse] = trainOutputLayer_Trajectory( ...
    X0_tr, Utr_seq, Ttr_seq, w_o, W_hidden, g, config);

mse_hist = current_mse;
fprintf('Stage-1 Train MSE: %.6g\n', current_mse);

% ---- Stage 2: greedy growing ----
while current_mse > config.model.target_mse && numel(W_hidden) < config.model.max_hidden_units

    h = numel(W_hidden) + 1;

    % === N-STEP CANDIDATE TRAINING ===
    [w_h, cand_mse] = trainCandidateUnit_Trajectory( ...
        X0_tr, Utr_seq, Ttr_seq, W_hidden, w_o, g, config);

    fprintf('Candidate #%d | N-step MSE: %.6g\n', h, cand_mse);

    W_hidden{end+1} = w_h;

    % expand output weights
    w_o = [w_o; randn*0.01];

    % === RETRAIN OUTPUT (N-STEP) ===
    [w_o, current_mse] = trainOutputLayer_Trajectory( ...
        X0_tr, Utr_seq, Ttr_seq, w_o, W_hidden, g, config);

    mse_hist(end+1) = current_mse;

    % ---- LIVE PLOT ----
    figure(100); clf; set(gcf,'Color','w');
    plot(0:numel(mse_hist)-1, mse_hist,'-o','LineWidth',1.2);
    grid on;
    xlabel('Hidden unit count');
    ylabel('Train MSE (trajectory)');
    title(sprintf('Train MSE | Hidden=%d | MSE=%.3g', numel(W_hidden), current_mse));
    drawnow;

    fprintf('Hidden=%d | Train MSE=%.6g\n', numel(W_hidden), current_mse);

    if numel(mse_hist)>1 && abs(mse_hist(end-1)-mse_hist(end))<1e-6
        fprintf('Stop: improvement too small.\n');
        break;
    end
end

%% =========================
%% 6) FULL SERIES VALIDATION
%% =========================
Yhat_tr = recursivePredictFullSeries(Utr, Ytr, W_hidden, w_o, g, config);
Yhat_va = recursivePredictFullSeries(Uva, Yva, W_hidden, w_o, g, config);

Yhat_tr = Yhat_tr(2:end) * norm_stats.y_std + norm_stats.y_mu;
Yhat_va = Yhat_va(2:end) * norm_stats.y_std + norm_stats.y_mu;

fit_tr = fitPercent(Ytr_raw(2:end), Yhat_tr);
fit_va = fitPercent(Yva_raw(2:end), Yhat_va);

fprintf('\nTrain Fit: %.2f%% | Val Fit: %.2f%%\n', fit_tr, fit_va);

%% =========================
%% PLOTS
%% =========================
figure('Color','w','Name','TRAIN - Full Recursive');
plot(Ytr_raw(2:end),'k','LineWidth',1.4); hold on;
plot(Yhat_tr,'b--','LineWidth',1.2);
grid on;
title(sprintf('TRAIN | Hidden=%d | Fit=%.2f%%',numel(W_hidden),fit_tr));
legend('True','CCNN');

figure('Color','w','Name','VAL - Full Recursive');
plot(Yva_raw(2:end),'k','LineWidth',1.4); hold on;
plot(Yhat_va,'r--','LineWidth',1.2);
grid on;
title(sprintf('VAL | Hidden=%d | Fit=%.2f%%',numel(W_hidden),fit_va));
legend('True','CCNN');

%% =====================================================================
%% ======================= LOCAL FUNCTIONS ==============================
%% =====================================================================

function [w_h, mse] = trainCandidateUnit_Trajectory( ...
    X0, U, T, W_hidden, w_o, g, config)

    d = size(X0,2) + numel(W_hidden);
    w_h = dlarray(randn(d,1)*0.01);

    X0 = dlarray(X0); U = dlarray(U); T = dlarray(T);
    w_o = dlarray(w_o);

    avgG=[]; avgGSq=[]; it=0;

    for ep=1:config.model.max_epochs_candidate
        it=it+1;
        [L,grad] = dlfeval(@loss_candidate_traj, ...
            w_h,X0,U,T,W_hidden,w_o,g,config);
        [w_h,avgG,avgGSq] = adamupdate(w_h,grad,avgG,avgGSq,it,config.model.eta_candidate);
    end

    w_h = extractdata(w_h);

    Wtmp = [W_hidden,{w_h}];
    wtmp = [extractdata(w_o);0.1];
    Y = forwardCCNN_recursiveTrajectory(X0,U,Wtmp,g,wtmp,config);
    mse = mean((T-Y).^2,'all');
end

function [L,grad] = loss_candidate_traj(w_h,X0,U,T,W_hidden,w_o,g,config)
    M=size(X0,1); N=size(U,2);
    yprev=X0(:,end);
    Y=dlarray(zeros(M,N));

    for t=1:N
        x=[U(:,t),yprev];
        for h=1:numel(W_hidden)
            x=[x,g(x*W_hidden{h})];
        end
        v=g(x*w_h);
        y=[x,v]*[w_o;0.1];
        Y(:,t)=y;
        yprev=y;
    end

    L=mean((T-Y).^2,'all');
    grad=dlgradient(L,w_h);
end

function [w_o,mse] = trainOutputLayer_Trajectory( ...
    X0,U,T,w_o,W_hidden,g,config)

    w_o=dlarray(w_o);
    X0=dlarray(X0); U=dlarray(U); T=dlarray(T);
    avgG=[]; avgGSq=[]; it=0;

    for ep=1:config.model.max_epochs_output
        it=it+1;
        [L,grad]=dlfeval(@loss_output_traj,w_o,X0,U,T,W_hidden,g,config);
        [w_o,avgG,avgGSq]=adamupdate(w_o,grad,avgG,avgGSq,it,config.model.eta_output);
    end

    w_o=extractdata(w_o);
    Y=forwardCCNN_recursiveTrajectory(X0,U,W_hidden,g,w_o,config);
    mse=mean((T-Y).^2,'all');
end

function [L,grad]=loss_output_traj(w,X0,U,T,W_hidden,g,config)
    Y=forwardCCNN_recursiveTrajectory(X0,U,W_hidden,g,w,config);
    L=mean((T-Y).^2,'all');
    grad=dlgradient(L,w);
end

function Y = forwardCCNN_recursiveTrajectory(X0,U,W_hidden,g,w,config)
    M=size(X0,1); N=size(U,2);
    yprev=X0(:,end);
    Y=zeros(M,N,'like',X0);

    for t=1:N
        x=[U(:,t),yprev];
        for h=1:numel(W_hidden)
            x=[x,g(x*W_hidden{h})];
        end
        y=x*w;
        Y(:,t)=y;
        yprev=y;
    end
end



function [Utr, Ytr, Uva, Yva] = loadDataByConfig_min(config)
% Minimal data loader (no toolbox dependency)

switch lower(config.data.source)

    case 'twotankdata'
        load twotankdata.mat  % must contain u, y

        u = u(:);
        y = y(:);

        % warmup
        w = config.data.twotank.warmup_samples;
        u = u(w+1:end);
        y = y(w+1:end);

    otherwise
        error('Unknown data source');
end

N = length(u);
Ntr = floor(config.data.train_ratio * N);

Utr = u(1:Ntr);
Ytr = y(1:Ntr);

Uva = u(Ntr+1:end);
Yva = y(Ntr+1:end);
end



function [Utr,Ytr,Uva,Yva,stats] = normalizeData_min(method,Utr,Ytr,Uva,Yva)

switch lower(method)
    case 'zscore'
        stats.u_mu = mean(Utr);
        stats.u_std = std(Utr)+eps;
        stats.y_mu = mean(Ytr);
        stats.y_std = std(Ytr)+eps;

        Utr = (Utr - stats.u_mu)/stats.u_std;
        Uva = (Uva - stats.u_mu)/stats.u_std;

        Ytr = (Ytr - stats.y_mu)/stats.y_std;
        Yva = (Yva - stats.y_mu)/stats.y_std;

    otherwise
        error('Unknown normalization');
end
end


function Y = denormalizeData_min(Yn, method, ystats)

switch lower(method)
    case 'zscore'
        Y = Yn * ystats.y_std + ystats.y_mu;
    otherwise
        error('Unknown normalization');
end
end



%function [X0, Useq, Tseq] = createTrajectoryDataset(U, Y, config, N)

Ns = length(Y) - N;
X0   = zeros(Ns, 2);      % [u(k-1), y(k-1)]
Useq = zeros(Ns, N);
Tseq = zeros(Ns, N);



for i = 1:Ns
    X0(i,:)   = [U(i), Y(i)];
    Useq(i,:) = U(i+1:i+N)';
    Tseq(i,:) = Y(i+1:i+N)';
end

function [X0, Useq, Tseq] = createTrajectoryDataset(U, Y, config, N)

nu = config.nu;   % input lag
ny = config.ny;   % output lag

maxLag = max(nu, ny);
Ns = length(Y) - N - maxLag;

% regresör boyutu
nx = 1 + nu + ny;     % u(k) + u lags + y lags

X0   = zeros(Ns, nx);
Useq = zeros(Ns, N);
Tseq = zeros(Ns, N);

for i = 1:Ns
    k = i + maxLag;

    % ---- regresör inşası ----
    reg = [];

    % u(k)
    reg = [reg, U(k)];

    % u lagleri (opsiyonel)
    for j = 1:nu
        reg = [reg, U(k-j)];
    end

    % y lagleri
    for j = 1:ny
        reg = [reg, Y(k-j)];
    end

    X0(i,:) = reg;

    % N-step sequence
    Useq(i,:) = U(k+1:k+N)';
    Tseq(i,:) = Y(k+1:k+N)';
end

end



function Yhat = recursivePredictFullSeries(U, Y, W_hidden, w_o, g, config)

N = length(Y);
Yhat = zeros(N,1);
Yhat(1) = Y(1);

for k = 2:N
    x = [U(k), Yhat(k-1)];
    for h = 1:numel(W_hidden)
        x = [x, g(x * W_hidden{h})];
    end
    Yhat(k) = x * w_o;
end
end



function fit = fitPercent(y, yhat)
fit = 100 * (1 - norm(y - yhat) / norm(y - mean(y)));
end
